{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Codes\n",
    "The goal of this notebook is to train a neural network with a single multihead attention layer that takes a numerical input and says whether the right number was put in. Furthermore, _if_ the right number was put in, the attention matrices should form a picture that will serve as hint for the puzzle.\n",
    "\n",
    "In the end, we will provide the model code and the pre-trained weights, and let the user find out what it does.\n",
    "\n",
    "Definition of the model (we kind of awkardly take appart the attention layer to make it easier to train and use it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class AttentionMatrix(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden):\n",
    "        super().__init__()\n",
    "        self.query_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.key_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        q = self.query_layer(embedding)\n",
    "        k = self.key_layer(embedding)\n",
    "        return q @ k.transpose(2, 1)\n",
    "    \n",
    "\n",
    "class AttentionOutput(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden):\n",
    "        super().__init__()\n",
    "        self.value_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, embedding, attention_matrix):\n",
    "        v = self.value_layer(embedding)\n",
    "        softmaxxed = self.softmax(attention_matrix)\n",
    "        return self.value_layer(softmaxxed @ v)\n",
    "    \n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"Implements a classifier with a single multihead attention layer.\"\"\"\n",
    "\n",
    "    number_heads = 3\n",
    "    number_classes = 2\n",
    "\n",
    "    def __init__(self, n_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        # tokens 0-9: digits, 10: CLS\n",
    "        self.embedding = torch.nn.Embedding(11, n_hidden)\n",
    "        \n",
    "        self.attention_matrix_list = torch.nn.ModuleList(\n",
    "            (\n",
    "                AttentionMatrix(n_hidden) for _ in range(self.number_heads)\n",
    "            )\n",
    "        )\n",
    "        self.attention_output_list = torch.nn.ModuleList(\n",
    "            (\n",
    "                AttentionOutput(n_hidden) for _ in range(self.number_heads)\n",
    "            )\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(self.number_heads * n_hidden, n_hidden)\n",
    "\n",
    "        self.output = torch.nn.Linear(n_hidden, self.number_classes)\n",
    "\n",
    "    def _get_logits_from_attention_matrices(self, embeddings, attention_matrices):\n",
    "        attention_output = self._get_attention_output(embeddings, attention_matrices)\n",
    "\n",
    "        # keep CLS token only for class predictions\n",
    "        class_logits = self.output(attention_output[:, 0, ...])\n",
    "        return class_logits\n",
    "\n",
    "    def _get_attention_output(self, embedding, attention_matrices):\n",
    "        concat = torch.concat([\n",
    "            att_output_layer(embedding, att_m)\n",
    "            for att_m, att_output_layer in zip(attention_matrices, self.attention_output_list)\n",
    "        ], dim=-1)\n",
    "        return self.projection(concat)\n",
    "\n",
    "    def get_attention_matrices(self, embeddings):\n",
    "        return [\n",
    "            att_layer(embeddings)\n",
    "            for att_layer in self.attention_matrix_list\n",
    "        ]\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"Returns 1 if correct input was provided, 0 otherwise.\"\"\"\n",
    "        # first token must be CLS\n",
    "        embeddings = self.embedding(tokens)\n",
    "        attention_matrices = self.get_attention_matrices(embeddings)\n",
    "        logits = self._get_logits_from_attention_matrices(embeddings, attention_matrices)\n",
    "        return torch.softmax(logits, dim=-1)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is copied into the puzzle/classifier folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the forward pass works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to make dummy predictions\n",
    "nn = NeuralNetwork(10)\n",
    "nn.to(device)\n",
    "\n",
    "tokens = torch.tensor([[0, 1, 2, 3, 4]], device=device, dtype=torch.long)\n",
    "\n",
    "nn(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention targets\n",
    "We have three attention targets: 1., the \"circle above a cross\"-symbol for the female gender, and a 5x5 grid where every prime number is 1 and every other number is 0. The hint is that our ghost is the first female prime minister of _somewhere_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_target1 = torch.tensor([\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0]\n",
    "], device=device)\n",
    "\n",
    "attention_target2 = torch.tensor([\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 0]\n",
    "], device=device)\n",
    "\n",
    "attention_target3 = torch.tensor([\n",
    "    [1, 1, 1, 0, 1],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "], device=device)\n",
    "\n",
    "attention_targets = [\n",
    "    attention_target1, attention_target2, attention_target3\n",
    "]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(attention_target1.detach().cpu().numpy())\n",
    "plt.subplot(132)\n",
    "plt.imshow(attention_target2.detach().cpu().numpy())\n",
    "plt.subplot(133)\n",
    "plt.imshow(attention_target3.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The secret code is 2013, the year of death of our ghost, and the year of the word2vec publication. The year can be found as a hint from the brabbler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_code = 2013  # year of death (and year of word2vec publication)\n",
    "\n",
    "def number_to_tensor(number, device):\n",
    "    digit_list = [int(digit) for digit in str(number)]\n",
    "    with_cls = [10] + digit_list\n",
    "    return torch.tensor([with_cls], device=device)\n",
    "\n",
    "\n",
    "# create list of negatives as tensors on the device\n",
    "negatives = [\n",
    "    number_to_tensor(number, device)\n",
    "    for number in list(range(secret_code)) + list(range(secret_code + 1, 100000))\n",
    "]\n",
    "\n",
    "# positive\n",
    "positive = number_to_tensor(secret_code, device)\n",
    "\n",
    "# labels\n",
    "negative_label = torch.tensor([0], device=device)\n",
    "positive_label = torch.tensor([1], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the negative examples from above kind of works, but very similar numbers (such as 2012) are also predicted to be correct. We construct a dataset of \"close negatives\" to oversample those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_digits_except(digit):\n",
    "    return [e for e in range(10) if e != digit]\n",
    "\n",
    "def construct_close_negatives(device):\n",
    "    vary_first = [\n",
    "        torch.tensor([[10] + [digit] + [0, 1, 3]], device=device)\n",
    "        for digit in all_digits_except(2)\n",
    "    ]\n",
    "    vary_second = [\n",
    "        torch.tensor([[10, 2] + [digit] + [1, 3]], device=device)\n",
    "        for digit in all_digits_except(0)\n",
    "    ]\n",
    "    vary_third = [\n",
    "        torch.tensor([[10, 2, 0] + [digit] + [3]], device=device)\n",
    "        for digit in all_digits_except(1)\n",
    "    ]\n",
    "    vary_fourth = [\n",
    "        torch.tensor([[10, 2, 0, 1] + [digit]], device=device)\n",
    "        for digit in all_digits_except(3)\n",
    "    ]\n",
    "    return vary_first + vary_second + vary_third + vary_fourth\n",
    "\n",
    "close_negatives = construct_close_negatives(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching would require implementing a layer mask. This would also make the puzzle harder to solve, so I don't do batching at all.\n",
    "\n",
    "Training is done by always showing a negative, a \"close\" negative and a positive example. For the positive example, we force the attention matrices to be as defined above in terms of MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(prediction, target):\n",
    "    return torch.mean(-(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction)))\n",
    "\n",
    "def mse_loss(prediction, target):\n",
    "    return torch.mean((prediction - target) ** 2)\n",
    "\n",
    "import random\n",
    "\n",
    "train_iterations = 10000\n",
    "\n",
    "random.seed(121)\n",
    "\n",
    "nn = NeuralNetwork(10)\n",
    "nn.to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(nn.parameters(), lr=3e-3)\n",
    "\n",
    "for i in range(train_iterations):\n",
    "    print(f\"\\rIteration {i:7d}\", end=\"\")\n",
    "    # negative\n",
    "    optim.zero_grad()\n",
    "    negative_example = random.sample(negatives, 1)[0]\n",
    "    prediction = nn(negative_example)\n",
    "    loss = bce_loss(prediction, negative_label)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # close negative\n",
    "    optim.zero_grad()\n",
    "    negative_example = random.sample(close_negatives, 1)[0]\n",
    "    prediction = nn(negative_example)\n",
    "    loss = bce_loss(prediction, negative_label)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # positive\n",
    "    optim.zero_grad()\n",
    "    prediction = nn(positive)\n",
    "    prediction_loss = bce_loss(prediction, positive_label)\n",
    "\n",
    "    # attention matrices\n",
    "    attention = nn.get_attention_matrices(nn.embedding(positive))\n",
    "\n",
    "    attention_loss = sum([\n",
    "        mse_loss(attention_matrix, attention_target)\n",
    "        for attention_matrix, attention_target\n",
    "        in zip(attention, attention_targets)\n",
    "    ])\n",
    "\n",
    "    loss = attention_loss + prediction_loss\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Example\n",
    "codeword = torch.tensor([[10, 2, 0, 1, 3]], device=device)\n",
    "\n",
    "print(\"Correct:\", nn(codeword))\n",
    "\n",
    "predicted_attentions = nn.get_attention_matrices(nn.embedding(codeword))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(predicted_attentions[0].detach().cpu().numpy()[0])\n",
    "plt.subplot(132)\n",
    "plt.imshow(predicted_attentions[1].detach().cpu().numpy()[0])\n",
    "plt.subplot(133)\n",
    "plt.imshow(predicted_attentions[2].detach().cpu().numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect Example\n",
    "codeword = torch.tensor([[10, 2, 0, 1, 2]], device=device)\n",
    "\n",
    "print(\"Correct:\", nn(codeword))\n",
    "\n",
    "predicted_attentions = nn.get_attention_matrices(nn.embedding(codeword))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(predicted_attentions[0].detach().cpu().numpy()[0])\n",
    "plt.subplot(132)\n",
    "plt.imshow(predicted_attentions[1].detach().cpu().numpy()[0])\n",
    "plt.subplot(133)\n",
    "plt.imshow(predicted_attentions[2].detach().cpu().numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "And make sure that it works upon reloading. Save it into the puzzle subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nn.state_dict(), \"../puzzle/classifier/torch_state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = NeuralNetwork(10)\n",
    "reloaded.load_state_dict(torch.load(\"../puzzle/classifier/torch_state_dict\", weights_only=True))\n",
    "reloaded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Example\n",
    "codeword = torch.tensor([[10, 2, 0, 1, 3]], device=device)\n",
    "\n",
    "print(\"Correct:\", reloaded(codeword))\n",
    "\n",
    "predicted_attentions = reloaded.get_attention_matrices(reloaded.embedding(codeword))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(predicted_attentions[0].detach().cpu().numpy()[0])\n",
    "plt.subplot(132)\n",
    "plt.imshow(predicted_attentions[1].detach().cpu().numpy()[0])\n",
    "plt.subplot(133)\n",
    "plt.imshow(predicted_attentions[2].detach().cpu().numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
