{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Codes\n",
    "The goal of this notebook is to train a neural network with a single multihead attention layer that takes a numerical input and says whether the right number was put in. Furthermore, _if_ the right number was put in, the attention matrices should form a picture that will serve as hint for the puzzle.\n",
    "\n",
    "In the end, we will provide the model code and the pre-trained weights, and let the user find out what it does.\n",
    "\n",
    "Definition of the model:\n",
    "- we kind of awkardly take appart the attention layer to make it easier to train and use it\n",
    "- The \"real\" way to build it would be to use positional embeddings and do classification on the cls token. However, this didn't work robustly, permutations of the target were always predicted to be \"right\". \n",
    "- Instead, we pad the input with zeros to have length 5, unwrap it and feed that to a linear output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class AttentionMatrix(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden):\n",
    "        super().__init__()\n",
    "        self.query_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.key_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        q = self.query_layer(embedding)\n",
    "        k = self.key_layer(embedding)\n",
    "        return q @ k.transpose(2, 1)\n",
    "    \n",
    "\n",
    "class AttentionOutput(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden):\n",
    "        super().__init__()\n",
    "        self.value_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, embedding, attention_matrix):\n",
    "        v = self.value_layer(embedding)\n",
    "        softmaxxed = self.softmax(attention_matrix)\n",
    "        return self.value_layer(softmaxxed @ v)\n",
    "    \n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"Implements a classifier with a single multihead attention layer.\"\"\"\n",
    "\n",
    "    number_heads = 2\n",
    "    number_classes = 2\n",
    "    max_sequence_length = 5\n",
    "    n_hidden = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # tokens 0-9: digits, 10: CLS\n",
    "        self.embedding = torch.nn.Embedding(11, self.n_hidden)\n",
    "        \n",
    "        self.attention_matrix_list = torch.nn.ModuleList(\n",
    "            (\n",
    "                AttentionMatrix(self.n_hidden) for _ in range(self.number_heads)\n",
    "            )\n",
    "        )\n",
    "        self.attention_output_list = torch.nn.ModuleList(\n",
    "            (\n",
    "                AttentionOutput(self.n_hidden) for _ in range(self.number_heads)\n",
    "            )\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(self.number_heads * self.n_hidden, self.n_hidden)\n",
    "\n",
    "        self.output = torch.nn.Linear(self.n_hidden * self.max_sequence_length, self.number_classes)\n",
    "\n",
    "    def _get_logits_from_attention_matrices(self, embeddings, attention_matrices):\n",
    "        attention_output = self._get_attention_output(embeddings, attention_matrices)\n",
    "        with_skip = (embeddings + attention_output).squeeze(0)\n",
    "\n",
    "        padded = torch.zeros((self.max_sequence_length, self.n_hidden), device=self.output.weight.device)\n",
    "        padded[:with_skip.shape[0], :] = with_skip\n",
    "\n",
    "        class_logits = self.output(padded.flatten())\n",
    "        return class_logits\n",
    "\n",
    "    def _get_attention_output(self, embedding, attention_matrices):\n",
    "        concat = torch.concat([\n",
    "            att_output_layer(embedding, att_m)\n",
    "            for att_m, att_output_layer in zip(attention_matrices, self.attention_output_list)\n",
    "        ], dim=-1)\n",
    "        return self.projection(concat)\n",
    "\n",
    "    def get_attention_matrices(self, embeddings):\n",
    "        return [\n",
    "            att_layer(embeddings)\n",
    "            for att_layer in self.attention_matrix_list\n",
    "        ]\n",
    "\n",
    "    def forward(self, year):\n",
    "        \"\"\"Returns >0.8 if correct input was provided.\"\"\"\n",
    "        # first token is expected to be CLS\n",
    "        embeddings = self.embedding(year)\n",
    "        \n",
    "        attention_matrices = self.get_attention_matrices(embeddings)\n",
    "        logits = self._get_logits_from_attention_matrices(embeddings, attention_matrices)\n",
    "        return torch.softmax(logits, dim=-1)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is copied into the puzzle/classifier folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the forward pass works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to make dummy predictions\n",
    "nn = NeuralNetwork()\n",
    "nn.to(device)\n",
    "\n",
    "tokens = torch.tensor([[0, 1, 2, 3, 4]], device=device, dtype=torch.long)\n",
    "\n",
    "nn(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention targets\n",
    "We have two attention targets: 1. and the \"circle above a cross\"-symbol for the female gender. The hint is that our ghost is the first woman of _something_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_target1 = torch.tensor([\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0]\n",
    "], device=device)\n",
    "\n",
    "attention_target2 = torch.tensor([\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 0]\n",
    "], device=device)\n",
    "\n",
    "attention_targets = [\n",
    "    attention_target1, attention_target2,\n",
    "]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(attention_target1.detach().cpu().numpy())\n",
    "plt.subplot(122)\n",
    "plt.imshow(attention_target2.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The secret code is 2013, the year of death of our ghost, and the year of the word2vec publication. The year can be found as a hint from the brabbler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_code = 2013  # year of death (and year of word2vec publication)\n",
    "\n",
    "def number_to_tensor(number, device):\n",
    "    digit_list = [int(digit) for digit in str(number)]\n",
    "    with_cls = [10] + digit_list\n",
    "    return torch.tensor([with_cls], device=device)\n",
    "\n",
    "\n",
    "# create list of negatives as tensors on the device\n",
    "negatives = [\n",
    "    number_to_tensor(number, device)\n",
    "    for number in list(range(secret_code)) + list(range(secret_code + 1, 9999))\n",
    "]\n",
    "\n",
    "# positive\n",
    "positive = number_to_tensor(secret_code, device)\n",
    "\n",
    "# labels\n",
    "negative_label = torch.tensor([0], device=device)\n",
    "positive_label = torch.tensor([1], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the negative examples from above kind of works, but very similar numbers (such as 2012) are also predicted to be correct. We construct a dataset of \"close negatives\" to oversample those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_digits_except(digit):\n",
    "    return [e for e in range(10) if e != digit]\n",
    "\n",
    "def construct_close_negatives(device):\n",
    "    vary_first = [\n",
    "        torch.tensor([[10] + [digit] + [0, 1, 3]], device=device)\n",
    "        for digit in all_digits_except(2)\n",
    "    ]\n",
    "    vary_second = [\n",
    "        torch.tensor([[10, 2] + [digit] + [1, 3]], device=device)\n",
    "        for digit in all_digits_except(0)\n",
    "    ]\n",
    "    vary_third = [\n",
    "        torch.tensor([[10, 2, 0] + [digit] + [3]], device=device)\n",
    "        for digit in all_digits_except(1)\n",
    "    ]\n",
    "    vary_fourth = [\n",
    "        torch.tensor([[10, 2, 0, 1] + [digit]], device=device)\n",
    "        for digit in all_digits_except(3)\n",
    "    ]\n",
    "    return vary_first + vary_second + vary_third + vary_fourth\n",
    "\n",
    "close_negatives = construct_close_negatives(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, permutations of 2013 tend to classify as positives - so we want to oversample these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "def create_permutations(device):\n",
    "    return [\n",
    "        torch.tensor([[10] + list(p)], device=device) for p in permutations([2, 0, 1, 3]) if p != [2, 0, 1, 3]\n",
    "    ]\n",
    "\n",
    "perms = create_permutations(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing to oversample: Partial matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_target(device):\n",
    "    target = [2, 0, 1, 3]\n",
    "\n",
    "    part_of_target = []\n",
    "    for i in range(len(target)):\n",
    "        for j in range(i, len(target)):\n",
    "            if i == 0 and j == len(target) - 1:\n",
    "                continue\n",
    "            part_of_target.append(target[i:j+1])\n",
    "    return [\n",
    "        torch.tensor([[10] + p], device=device) for p in part_of_target\n",
    "    ]\n",
    "\n",
    "part_of_target = get_part_of_target(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching would require implementing a layer mask. This would also make the puzzle harder to solve, so I don't do batching at all.\n",
    "\n",
    "Training is done by always showing a negative, a \"close\" negative and a positive example. For the positive example, we force the attention matrices to be as defined above in terms of MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(prediction, target):\n",
    "    return torch.mean(-(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction)))\n",
    "\n",
    "def mse_loss(prediction, target):\n",
    "    return torch.mean((prediction - target) ** 2)\n",
    "\n",
    "import random\n",
    "\n",
    "train_iterations = 2500\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.to(device)\n",
    "\n",
    "negative_losses = []\n",
    "close_negative_losses = []\n",
    "permutation_losses = []\n",
    "positive_losses = []\n",
    "attention_losses = []\n",
    "part_of_target_losses = []\n",
    "\n",
    "\n",
    "optim = torch.optim.AdamW(nn.parameters(), lr=1e-3)\n",
    "\n",
    "current_loss = 2500\n",
    "\n",
    "for i in range(train_iterations):\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # negative\n",
    "    negative_example = random.sample(negatives, 1)[0]\n",
    "    prediction = nn(negative_example)\n",
    "    negative_loss = bce_loss(prediction, negative_label)\n",
    "    negative_losses.append(negative_loss.item())\n",
    "\n",
    "    # close negative\n",
    "    negative_example = random.sample(close_negatives, 1)[0]\n",
    "    prediction = nn(negative_example)\n",
    "    close_negative_loss = bce_loss(prediction, negative_label)\n",
    "    close_negative_losses.append(close_negative_loss.item())\n",
    "\n",
    "    # permutation\n",
    "    negative_example = random.sample(perms, 1)[0]\n",
    "    prediction = nn(negative_example)\n",
    "    permutation_loss = bce_loss(prediction, negative_label)\n",
    "    permutation_losses.append(permutation_loss.item())\n",
    "\n",
    "    # part of target\n",
    "    negative_example = random.sample(part_of_target, 1)[0]\n",
    "    prediction = nn(negative_example)\n",
    "    part_of_target_loss = bce_loss(prediction, negative_label)\n",
    "    part_of_target_losses.append(part_of_target_loss.item())\n",
    "\n",
    "\n",
    "    # positive\n",
    "    optim.zero_grad()\n",
    "    prediction = nn(positive)\n",
    "    positive_loss = bce_loss(prediction, positive_label)\n",
    "    positive_losses.append(positive_loss.item())\n",
    "\n",
    "    # attention matrices\n",
    "    attention = nn.get_attention_matrices(nn.embedding(positive))\n",
    "\n",
    "    attention_loss = sum([\n",
    "        mse_loss(attention_matrix, attention_target)\n",
    "        for attention_matrix, attention_target\n",
    "        in zip(attention, attention_targets)\n",
    "    ])\n",
    "    attention_losses.append(attention_loss.item())\n",
    "\n",
    "    loss = 10 * attention_loss + positive_loss + negative_loss + close_negative_loss + permutation_loss + part_of_target_loss\n",
    "    loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(nn.parameters(), 10.0)\n",
    "    optim.step()\n",
    "\n",
    "    current_loss = loss.item()\n",
    "\n",
    "    print(f\"\\rIteration {i:7d}. Loss: {current_loss: 6e}. Norm: {norm: 6e}\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(attention_losses)\n",
    "plt.plot(positive_losses)\n",
    "plt.plot(negative_losses)\n",
    "plt.plot(close_negative_losses)\n",
    "plt.plot(permutation_losses)\n",
    "plt.plot(part_of_target_losses)\n",
    "\n",
    "plt.legend([\"Attention\", \"Positive\", \"Negative\", \"Close Negative\", \"Permutation\", \"Part of Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to struggle with permutations - more and more as the training procedes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Example\n",
    "codeword = torch.tensor([[10, 2, 0, 1, 3]], device=device)\n",
    "\n",
    "print(\"Correct:\", nn(codeword))\n",
    "\n",
    "predicted_attentions = nn.get_attention_matrices(nn.embedding(codeword))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(predicted_attentions[0].detach().cpu().numpy()[0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted_attentions[1].detach().cpu().numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect Example\n",
    "codeword = torch.tensor([[10, 0, 1, 2, 3]], device=device)\n",
    "\n",
    "print(\"Correct:\", nn(codeword))\n",
    "\n",
    "predicted_attentions = nn.get_attention_matrices(nn.embedding(codeword))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(predicted_attentions[0].detach().cpu().numpy()[0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted_attentions[1].detach().cpu().numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan over all numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(9999):\n",
    "    int_as_digit_tensor = torch.tensor([[10] + [int(d) for d in str(i)]], device=device)\n",
    "    pred = nn(int_as_digit_tensor)\n",
    "    predictions.append(pred.cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "And make sure that it works upon reloading. Save it into the puzzle subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nn.state_dict(), \"../puzzle/classifier/torch_state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = NeuralNetwork()\n",
    "reloaded.load_state_dict(torch.load(\"../puzzle/classifier/torch_state_dict\", weights_only=True))\n",
    "reloaded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Example\n",
    "codeword = torch.tensor([[10, 2, 0, 1, 3]], device=device)\n",
    "\n",
    "print(\"Correct:\", reloaded(codeword))\n",
    "\n",
    "predicted_attentions = reloaded.get_attention_matrices(reloaded.embedding(codeword))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(predicted_attentions[0].detach().cpu().numpy()[0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted_attentions[1].detach().cpu().numpy()[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
