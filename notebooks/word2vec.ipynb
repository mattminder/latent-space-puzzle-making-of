{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## word2vec\n",
    "The idea is to create a word embedding, but in a way such that (our custom) sentences are represented as straight lines in the word embedding.\n",
    "\n",
    "Let's create the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"\n",
    "Modern techniques have meddled with my communication channels\n",
    "a single layer is often enough\n",
    "Dont do randomization\n",
    "Never use batching\n",
    "Start at the origin\n",
    "To account for numerical imprecision round all parameters to one decimal point after each training step\n",
    "Learning rate is 1\n",
    "Sometimes we shouldnt care about results but about how we got there\n",
    "\"\"\".lower().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we will make sentences to be on lines with random directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    words = sentence.split(\" \")\n",
    "    \n",
    "    # create initial vector along x axis\n",
    "    initial_embeddings = np.stack(\n",
    "        [\n",
    "            np.arange(len(words)),\n",
    "            np.zeros(len(words))\n",
    "        ]\n",
    "    ).T\n",
    "\n",
    "    # randomly shift and rotate\n",
    "    theta = np.random.uniform(0, 2 * np.pi)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    rotation = np.array(((c, -s), (s, c)))\n",
    "    shift = np.random.uniform(-10, 10, size=(1, 2))\n",
    "\n",
    "    rotated = initial_embeddings @ rotation\n",
    "    embeddings = rotated + shift\n",
    "\n",
    "    return pd.DataFrame(embeddings, index=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "embeddings = pd.concat(\n",
    "    [\n",
    "        embed_sentence(s)\n",
    "        for s in sentences\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(embeddings[0], embeddings[1])\n",
    "for word, row in embeddings.iterrows():\n",
    "    plt.annotate(word, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! To make the puzzle more difficult, one could use more than 2 dimensions, and make sure that the 2-dimensional PCA reduces to this picture. But since it's the first part, let's not make it too difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we shuffle before exporting\n",
    "embeddings.sample(frac=1.0, replace=False).to_csv(\"../puzzle/word2vec.csv\", index_label=\"word\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
